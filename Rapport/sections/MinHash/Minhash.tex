\documentclass[../../main.tex]{subfiles}
\begin{document}
\section{Minwise and Maxwise Hashing}
The core of the clustering algorithm in this paper will be based on the concepts of minwise hashing as described in \cite{MinwiseIndependent}. Minwise hashing has repeatedly proven a powerful tool when comparing large sets of strings rapidly, especially for duplicate detection of long articles. The use of minwise hashing for rRNA sequences has already been done in \cite{MinhashMapreduce}, however the method of this paper will be extended by applying two methods of maxwise hashing as described in \cite{minmaxhash}.
\subsection{Introduction to Minwise Hashing}
Let there be two sets $A$ and $B$. To find the similarity between the two sets, minwise hashing uses is the Jaccard similarity measure, which is defined as
\begin{equation}\label{jaccard}
J(A,B)=\frac{|A\cap B|}{|A\cup B|}
\end{equation}
To increase the speed of calculating the Jaccard similarity, it however uses hash functions to find the value. Let us observe how the hash functions are used in this case.
\subsubsection{Min-wise Independency}
Let $H: U \rightarrow [r]$ be a class of hashfunctions. Then for any set $X\subseteq [r]$ and any $x \in X$ and let $h\in H$ be chosen uniformly at random, it is considered minwise independent if
\begin{equation}\label{minwise}
\mathrm{Pr}(h_{\min}(X)=h(x))=\frac{1}{|X|}
\end{equation}
where
$$
h_{\min}(X) = \min\{\forall x \in X, h(x)\} 
$$
Meaning that all elements in $X$ must have an equal probability of having the minimum value going through $h$. As seen in Eq. \ref{universalProb}, this probability is reachable using universal hash functions, which is a great increase in speed over perfect hashing functions.

\subsubsection{Min-wise sketch}
For two sets $A$ and $B$ it has been proven in \cite{protominwise} that Eq. \ref{minwise} can be linked to the Jaccard similarity in Eq. \ref{jaccard} as
\begin{equation}\label{minwisejaccard}
\mathrm{Pr}(h_{\min}(A)=h_{\min}(B))=\frac{|A\cap B|}{|A\cup B|}
\end{equation}

For a random set $S_1$, we may create a table of random $h_{\min,i},i=1,..,k$ such that
$$
\hat{S_1} = \{ h_{\min,1}(S_1),h_{\min,2}(S_1),...,h_{\min,k}(S_1)\}
$$

We may then compute the similarity of two sets $\hat{S_1}$ and $\hat{S_2}$ defined by the above equation as
\begin{equation}\label{jacsketch}
J(A,B)=\frac{1}{k}\cdot \sum_{i=1}^k (h_{\min,i}(S_1) = h_{\min,i}(S_2))
\end{equation}
where
$$
(h_{\min,i}(S_1) = h_{\min,i}(S_1)) = \left\{ \begin{array}{ll}
												1, \ \ h_{\min,i}(S_1)=h_{\min,i}(S_2)\\
												0, \ \ otherwise
											  \end{array}\right.
$$
which is called the \textbf{minwise sketch}. As we see from Eq. \ref{universalProb}, there will be a slight error in the calculation of $h_min$. Therefore we must see what influence the size of $k$ will have on the error. A proof of the error using Chernoff Bounds\footnote{A probablistic method to find the exponentially decreasing bounds between two independent variates.} is found in \cite{errorMinhash}, shows that the relation between $k$ and $\epsilon$, the error, is 
$$
k=O\left(\log \frac{1}{\epsilon}\right)
$$
Thus, k influences the error inversely exponentially, meaning that $k\approx 100$ should guarantee very small error.

\subsection{Max-wise hashing}

The aforementioned modification is one inspired by the method in the paper \cite{minmaxhash}. It is an extension of the minwise sketch where in addition to using the minwise independent sets, we add the maxwise independent set too. Very literally, this means that instead of using the minimum hashvalue, we use the maximal hashvalue such that a set $X$ is said to be maxwise independent if
\begin{equation}\label{maxwise}
\mathrm{Pr}(h_{\max}(X)=h(x))=\frac{1}{|X|}, h_{\max}=\max\{\forall x \in X, h(x)\} 
\end{equation}

for any $x\in X$. The Jaccard similarity measure for two sets $A$ and $B$ is
\begin{equation}\label{maxwisejaccard}
\mathrm{Pr}(h_{\max}(A)=h_{\max}(B))=\frac{|A\cap B|}{|A\cup B|}
\end{equation}
and finally for a random set $S_1$ we may create a table of random $h_{\max,i},i=1,...,k$ such that
$$
\tilde{S_1} = \{ h_{\max,1}(S_1),h_{\max,2}(S_1),...,h_{\max,k}(S_1)\}
$$
This sketch will function almost like the minwise sketch. It is first when combining the two sketches that they have interesting properties.
\subsection{Combining Max-wise and Min-wise}

There are two ways of combining the max-wise and the min-wise algorithm. One is the method in \cite{minmaxhash}, where they halve the amount of hashfunctions, so that for $i=1,..,k/2$
\begin{equation}\label{minmaxhalfjaccard}
J(A,B)=\frac{1}{K}\sum_{i=1}^{K/2}(h_{\min,i}(A) = h_{\min,i}(B) + h_{\max,i}(A) = h_{\max,i}(B))
\end{equation}
Let this method be called \textbf{Max-minwise halved sketch} (abbr. {\bf Mm½}). This method has been proven to be double as quick as the min-wise hashing, without loss of precision\cite{minmaxhash}. It is also shown in Lemma 2 in \cite{minmaxhash} that for $i=1,..,k/2$
$$
\mathrm{Pr}(h_{\min,i}(A) = h_{\min,i}(B) | h_{\max,i}(A) = h_{\max,i}(B)) = \frac{|A\cap B|-1}{|A\cup B| -1}
$$
Meaning that a collision between $h_{\min}$ and $h_{\max}$ is very unlikely.\\

Another method, which was developed in the course of this paper uses the following combination
\begin{equation}\label{minmaxjaccard}
J(A,B)=\frac{1}{k}\sum_{i=1}^{k}(h_{\min,i}(A) = h_{\min,i}(B) | h_{\max,i}(A) = h_{\max,i}(B))
\end{equation}
where
$$
h_{\min,i}(A) = h_{\min,i}(B) | h_{\max,i}(A) = h_{\max,i}(B) = \left\{ \begin{array}{ll}
												1, \ \ h_{\min,i}(S_1)=h_{\min,i}(S_2)\\
												1, \ \ h_{\max,i}(S_1)=h_{\max,i}(S_2)\\
												0, \ \ otherwise
											  \end{array}\right.
$$

Let this method be called \textbf{Max-minwise sketch} (abbr. {\bf Mm}). This also has finds the Jaccard similarity by the following proof:\\
\begin{equation}\label{proofofMinmax}
\begin{split}
\frac{1}{k}\sum_{i=1}^{k}(h_{\min,i}(A) = h_{\min,i}(B) | h_{\max,i}(A) = h_{\max,i}(B)) =\\\frac{1}{k}\sum_{i=1}^{k}(J(A,B) | J(A,B)) = J(A,B)|J(A,B) = J(A,B)
\end{split}
\end{equation}

the final three steps follow from Eq. \ref{jacsketch} and Eq. \ref{maxwisejaccard}. Therefore we see that this method also finds the jaccard similarity.\\

As one may have noted, the difference between \textbf{Mm½} and \textbf{Mm} is that the first runs only half as many times as the second for each comparison between two sets.

\end{document}

